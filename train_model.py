import os
import random
import torch
from torch import nn, optim
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader, random_split
from tqdm import tqdm
from datetime import datetime

# ===============================================================
# ğŸ”§ CONFIG
# ===============================================================
DATA_DIR = "dataset/train"
NUM_CLASSES = 5
NUM_EPOCHS = 2          # keep small for CPU testing
BATCH_SIZE = 16
LR = 1e-4
SEED = 42
PATIENCE = 3            # early stopping patience
MODEL_PATH = "classifier.pt"

# ===============================================================
# ğŸ¯ REPRODUCIBILITY
# ===============================================================
torch.manual_seed(SEED)
random.seed(SEED)
torch.cuda.manual_seed_all(SEED)

# ===============================================================
# ğŸ–¥ï¸ DEVICE
# ===============================================================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ–¥ï¸ Using device: {device}")

# ===============================================================
# ğŸ§ª DATA TRANSFORMS
# ===============================================================
train_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])

val_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])

# ===============================================================
# ğŸ“¦ DATASET
# ===============================================================
full_dataset = datasets.ImageFolder(DATA_DIR, transform=train_transform)

train_size = int(0.8 * len(full_dataset))
val_size = len(full_dataset) - train_size

train_dataset, val_dataset = random_split(
    full_dataset, [train_size, val_size]
)

# Override transform for validation
val_dataset.dataset.transform = val_transform

train_loader = DataLoader(
    train_dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    num_workers=2,
    pin_memory=True
)

val_loader = DataLoader(
    val_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    num_workers=2,
    pin_memory=True
)

print(f"ğŸ“Š Total: {len(full_dataset)} | Train: {train_size} | Val: {val_size}")

# ===============================================================
# ğŸ§  MODEL
# ===============================================================
model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)
in_features = model.fc.in_features

model.fc = nn.Sequential(
    nn.Linear(in_features, 512),
    nn.ReLU(inplace=True),
    nn.Dropout(0.3),
    nn.Linear(512, NUM_CLASSES)
)

model.to(device)

# ===============================================================
# âš™ï¸ LOSS & OPTIMIZER
# ===============================================================
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LR)

scaler = torch.cuda.amp.GradScaler(enabled=(device.type == "cuda"))

# ===============================================================
# ğŸš€ TRAINING LOOP
# ===============================================================
best_val_acc = 0.0
epochs_no_improve = 0

print("\nğŸš€ Training started...\n")

for epoch in range(NUM_EPOCHS):
    model.train()
    train_loss = 0.0
    correct = 0
    total = 0

    loop = tqdm(train_loader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS}")
    for inputs, labels in loop:
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()

        with torch.cuda.amp.autocast(enabled=(device.type == "cuda")):
            outputs = model(inputs)
            loss = criterion(outputs, labels)

        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        train_loss += loss.item()
        _, preds = torch.max(outputs, 1)
        correct += (preds == labels).sum().item()
        total += labels.size(0)

        loop.set_postfix(loss=loss.item())

    train_acc = 100 * correct / total
    avg_train_loss = train_loss / len(train_loader)

    # ===============================================================
    # âœ… VALIDATION
    # ===============================================================
    model.eval()
    val_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, preds = torch.max(outputs, 1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)

    val_acc = 100 * correct / total
    avg_val_loss = val_loss / len(val_loader)

    print(
        f"\nğŸ“˜ Epoch {epoch+1}"
        f" | Train Loss: {avg_train_loss:.4f}"
        f" | Train Acc: {train_acc:.2f}%"
        f" | Val Loss: {avg_val_loss:.4f}"
        f" | Val Acc: {val_acc:.2f}%"
    )

    # ===============================================================
    # ğŸ’¾ SAVE BEST MODEL
    # ===============================================================
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        epochs_no_improve = 0

        torch.save({
            "model_state_dict": model.state_dict(),
            "optimizer_state_dict": optimizer.state_dict(),
            "num_classes": NUM_CLASSES,
            "architecture": "resnet18",
            "val_accuracy": best_val_acc,
            "timestamp": datetime.utcnow().isoformat()
        }, MODEL_PATH)

        print(f"ğŸ’¾ Best model saved! (Val Acc: {best_val_acc:.2f}%)")
    else:
        epochs_no_improve += 1
        if epochs_no_improve >= PATIENCE:
            print("â¹ï¸ Early stopping triggered")
            break

print("\nğŸ¯ Training complete")
print(f"ğŸ† Best Validation Accuracy: {best_val_acc:.2f}%")
print(f"ğŸ“¦ Model saved as: {MODEL_PATH}")
